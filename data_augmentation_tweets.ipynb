{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "OnKYY7_yaX78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n",
        "\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:06.358698Z",
          "iopub.execute_input": "2023-05-08T19:55:06.359079Z",
          "iopub.status.idle": "2023-05-08T19:55:06.816005Z",
          "shell.execute_reply.started": "2023-05-08T19:55:06.359047Z",
          "shell.execute_reply": "2023-05-08T19:55:06.815206Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOw0ZP7PAEeX",
        "outputId": "e1ccba7c-b2cb-4856-b639-3b360f8591b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tweets       class\n",
            "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
            "1  #sarcasm for #people who don't understand #diy...  figurative\n",
            "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
            "3  @wilw Why do I get the feeling you like games?...  figurative\n",
            "4  -@TeacherArthurG @rweingarten You probably jus...  figurative\n",
            "                                              tweets       class\n",
            "0  no one ever predicted this was going to happen...  figurative\n",
            "1  @Stooshie its as closely related as Andrews or...  figurative\n",
            "2  I find it ironic when Vegans say they love foo...  figurative\n",
            "3  Quick rt that throwing money vine I've not see...  figurative\n",
            "4  yep, keep adding me to your #devops lists.... ...  figurative\n",
            "(81408, 2)\n",
            "(8128, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle null data"
      ],
      "metadata": {
        "id": "fF60AFhBachx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.isnull().sum())\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "test_df.dropna(inplace=True)\n",
        "\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:06.817487Z",
          "iopub.execute_input": "2023-05-08T19:55:06.817967Z",
          "iopub.status.idle": "2023-05-08T19:55:06.875876Z",
          "shell.execute_reply.started": "2023-05-08T19:55:06.817940Z",
          "shell.execute_reply": "2023-05-08T19:55:06.874714Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l78jAdseAEeg",
        "outputId": "151067f9-a2f5-430d-92b7-b69961b39779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweets    0\n",
            "class     0\n",
            "dtype: int64\n",
            "tweets    2\n",
            "class     9\n",
            "dtype: int64\n",
            "tweets    0\n",
            "class     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine and check for class imbalance"
      ],
      "metadata": {
        "id": "0EDPwUmqahk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate train and test data\n",
        "df = pd.concat([train_df, test_df], axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:06.877758Z",
          "iopub.execute_input": "2023-05-08T19:55:06.878536Z",
          "iopub.status.idle": "2023-05-08T19:55:06.889030Z",
          "shell.execute_reply.started": "2023-05-08T19:55:06.878494Z",
          "shell.execute_reply": "2023-05-08T19:55:06.887853Z"
        },
        "trusted": true,
        "id": "QiW8_QIbAEeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(list(df[\"class\"])))\n",
        "df['class'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:06.891569Z",
          "iopub.execute_input": "2023-05-08T19:55:06.891970Z",
          "iopub.status.idle": "2023-05-08T19:55:06.957523Z",
          "shell.execute_reply.started": "2023-05-08T19:55:06.891938Z",
          "shell.execute_reply": "2023-05-08T19:55:06.956469Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqkaRAxRAEeh",
        "outputId": "79346b1b-b26d-4e59-d999-e7017204a408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['figurative' 'irony' 'regular' 'sarcasm']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "figurative    23282\n",
              "irony         23005\n",
              "sarcasm       22786\n",
              "regular       20454\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see there is slight imbalanced on the data i gonna augment it but first we gonna clean it"
      ],
      "metadata": {
        "id": "uImhLmGzAEei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(list(df[\"class\"])))\n",
        "print(df['class'].value_counts())\n",
        "print(df.shape)\n",
        "df = df.drop_duplicates(subset=['tweets'])\n",
        "print(np.unique(list(df[\"class\"])))\n",
        "print(df['class'].value_counts())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:06.958944Z",
          "iopub.execute_input": "2023-05-08T19:55:06.959424Z",
          "iopub.status.idle": "2023-05-08T19:55:07.076451Z",
          "shell.execute_reply.started": "2023-05-08T19:55:06.959384Z",
          "shell.execute_reply": "2023-05-08T19:55:07.075402Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV8MxF_1AEej",
        "outputId": "1a8c0837-916d-438e-b65a-4f2aef9b8eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['figurative' 'irony' 'regular' 'sarcasm']\n",
            "figurative    23282\n",
            "irony         23005\n",
            "sarcasm       22786\n",
            "regular       20454\n",
            "Name: class, dtype: int64\n",
            "(89527, 2)\n",
            "['figurative' 'irony' 'regular' 'sarcasm']\n",
            "figurative    22001\n",
            "regular       20427\n",
            "sarcasm       16955\n",
            "irony         14008\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the data by stopwords, and regex"
      ],
      "metadata": {
        "id": "X4y30mNuapQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def clean(tweet): \n",
        "    import re\n",
        "    import string\n",
        "    import nltk\n",
        "    \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    \n",
        "    #emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    tweet =  emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    # usernames mentions like \"@abc123\"\n",
        "    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n",
        "    tweet =  ment.sub(r'', tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # html tags\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    tweet = re.sub(html, '', tweet)\n",
        "    \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+','', tweet)\n",
        "        \n",
        "    #Punctuations and special characters\n",
        "    \n",
        "    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n",
        "    \n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    splits = tweet.split()\n",
        "    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    #tweet = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    tweet = ' '.join(splits)\n",
        "    \n",
        "    return tweet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:07.077921Z",
          "iopub.execute_input": "2023-05-08T19:55:07.078249Z",
          "iopub.status.idle": "2023-05-08T19:55:08.406513Z",
          "shell.execute_reply.started": "2023-05-08T19:55:07.078222Z",
          "shell.execute_reply": "2023-05-08T19:55:08.405425Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGw5c206AEel",
        "outputId": "dd56e397-e4e5-46cf-8fba-3dd98a041374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweets']= df['tweets'].apply((lambda x: clean(x))) \n",
        "print(\"Cleaned\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:55:08.407863Z",
          "iopub.execute_input": "2023-05-08T19:55:08.408744Z",
          "iopub.status.idle": "2023-05-08T19:57:35.582332Z",
          "shell.execute_reply.started": "2023-05-08T19:55:08.408712Z",
          "shell.execute_reply": "2023-05-08T19:57:35.581235Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9niXV6qAEeo",
        "outputId": "891724ac-c5dc-4251-f29e-61fcbb9a9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode the data to 0,1,2,3 for easier processing later down the line"
      ],
      "metadata": {
        "id": "5D-vlGRBav04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# assuming your class column is named 'class'\n",
        "le = LabelEncoder()\n",
        "df['class'] = le.fit_transform(df['class'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:57:35.583782Z",
          "iopub.execute_input": "2023-05-08T19:57:35.584110Z",
          "iopub.status.idle": "2023-05-08T19:57:35.606056Z",
          "shell.execute_reply.started": "2023-05-08T19:57:35.584082Z",
          "shell.execute_reply": "2023-05-08T19:57:35.605200Z"
        },
        "trusted": true,
        "id": "gXkBNRbnAEep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:57:35.607193Z",
          "iopub.execute_input": "2023-05-08T19:57:35.608103Z",
          "iopub.status.idle": "2023-05-08T19:57:35.617133Z",
          "shell.execute_reply.started": "2023-05-08T19:57:35.608070Z",
          "shell.execute_reply": "2023-05-08T19:57:35.616166Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3wuOUGlAEep",
        "outputId": "9334a590-3146-4f16-8f0f-58e17aeb3653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of                                                  tweets  class\n",
            "0     aware dirty step get money staylight staywhite...      0\n",
            "1               sarcasm people understand diy artattack      0\n",
            "2     dailymail readers sensible always shocker sarc...      0\n",
            "3                        get feeling like games sarcasm      0\n",
            "4                        probably missed text sarcastic      0\n",
            "...                                                 ...    ...\n",
            "8123  yes totally submit photos shitty online magazi...      3\n",
            "8124  test saturday thank uni sarcasm griffith unive...      3\n",
            "8125             listening misery disconcerting sarcasm      3\n",
            "8126                       go kind sarcasm standup4kids      3\n",
            "8127       shocked refs tcu vs minn game big 12 sarcasm      3\n",
            "\n",
            "[73391 rows x 2 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synonym augmentation"
      ],
      "metadata": {
        "id": "aL_jvSCYa30c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "            synonyms.add(synonym)\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def replace_with_synonyms(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    new_words=[]\n",
        "    for word in words:\n",
        "        synonyms = get_synonyms(word)\n",
        "        if len(synonyms) > 0:\n",
        "            new_words.append(synonyms[0])\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "augmented_data = []\n",
        "for index, row in df.iterrows():\n",
        "    text = row[\"tweets\"]\n",
        "    label = row[\"class\"]\n",
        "    augmented_text = replace_with_synonyms(text)\n",
        "    augmented_data.append((augmented_text, label))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T20:10:44.195503Z",
          "iopub.execute_input": "2023-05-08T20:10:44.195852Z",
          "iopub.status.idle": "2023-05-08T20:10:44.352514Z",
          "shell.execute_reply.started": "2023-05-08T20:10:44.195828Z",
          "shell.execute_reply": "2023-05-08T20:10:44.351186Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVa54pxdAEeq",
        "outputId": "cb65e0b9-20e2-44c8-b5e8-052a7f9ce085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Find the majority class\n",
        "class_counts = df['class'].value_counts()\n",
        "majority_class = class_counts.idxmax()\n",
        "num_samples = class_counts[majority_class]\n",
        "\n",
        "# Step 2: Find the minority classes\n",
        "minority_classes = class_counts.index[class_counts < num_samples]\n",
        "\n",
        "# Step 3: Calculate how many additional samples are needed for each minority class\n",
        "additional_samples = {}\n",
        "for c in minority_classes:\n",
        "    diff = num_samples - class_counts[c]\n",
        "    additional_samples[c] = diff\n",
        "\n",
        "# Step 4: Apply the synonym augmentation to the texts in the minority classes\n",
        "augmented_data = []\n",
        "for c in minority_classes:\n",
        "    class_df = df[df['class'] == c]\n",
        "    num_augmented = additional_samples[c]\n",
        "    for i in range(num_augmented):\n",
        "        index = random.randint(0, len(class_df) - 1)\n",
        "        text = class_df.iloc[index]['tweets']\n",
        "        augmented_text = replace_with_synonyms(text)\n",
        "        augmented_data.append((augmented_text, c))\n",
        "\n",
        "# Step 5: Combine the original dataset with the augmented samples and shuffle the rows\n",
        "augmented_df = pd.DataFrame(augmented_data, columns=['tweets', 'class'])\n",
        "balanced_df = pd.concat([df, augmented_df], axis=0)\n",
        "balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T20:00:45.022048Z",
          "iopub.execute_input": "2023-05-08T20:00:45.022600Z",
          "iopub.status.idle": "2023-05-08T20:00:45.047910Z",
          "shell.execute_reply.started": "2023-05-08T20:00:45.022558Z",
          "shell.execute_reply": "2023-05-08T20:00:45.046436Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWyWu6pAEer",
        "outputId": "764f5368-64fd-4078-e31a-5299b266e258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for imbalance"
      ],
      "metadata": {
        "id": "uX9Q7YHEa84K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(list(balanced_df[\"class\"])))\n",
        "print(balanced_df['class'].value_counts())\n",
        "print(balanced_df.head)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-08T19:57:36.395749Z",
          "iopub.status.idle": "2023-05-08T19:57:36.396138Z",
          "shell.execute_reply.started": "2023-05-08T19:57:36.395940Z",
          "shell.execute_reply": "2023-05-08T19:57:36.395957Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRjCmgScAEer",
        "outputId": "1a74fd1c-647e-4509-8c77-303c4199e8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3]\n",
            "0    22001\n",
            "2    22001\n",
            "3    22001\n",
            "1    22001\n",
            "Name: class, dtype: int64\n",
            "<bound method NDFrame.head of                                                   tweets  class\n",
            "0          deep insight clockmaking arrest story sarcasm      0\n",
            "1      office move looming already made start plan ah...      0\n",
            "2      subway created 5 dollar ft long anthem 5 years...      0\n",
            "3      hello please sign share petition get mentalhea...      2\n",
            "4      aust republican movement cofounder swears bear...      0\n",
            "...                                                  ...    ...\n",
            "87999  finally realize friends w drug addict rmr dadv...      2\n",
            "88000                    oh love made huge bitch sarcasm      3\n",
            "88001  rundown gop candidates 2015 september debate l...      2\n",
            "88002  glad fairandbalanced debate hosted megynkelly ...      3\n",
            "88003  ‘consumers fraudsters’ magically banks pasa an...      3\n",
            "\n",
            "[88004 rows x 2 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Done"
      ],
      "metadata": {
        "id": "FDOrlZgybAs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.to_csv(\"augmented_data_BALANCED.csv\", index=False)\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "1qcamU4hH9vJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211ece26-c5d4-4da3-8416-d3053766e654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    }
  ]
}